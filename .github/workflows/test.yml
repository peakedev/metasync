name: Test Pipeline

on:
  push:
    branches:
      - '**'
  pull_request:
    branches:
      - '**'
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      checks: write
      pull-requests: write
    
    services:
      mongo:
        image: mongo:7.0
        ports:
          - 27017:27017
        options: >-
          --health-cmd "mongosh --eval 'db.runCommand({ ping: 1 })'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'
      
      - name: Install MongoDB tools
        run: |
          wget -qO - https://www.mongodb.org/static/pgp/server-7.0.asc | sudo apt-key add -
          echo "deb [ arch=amd64,arm64 ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list
          sudo apt-get update
          sudo apt-get install -y mongodb-mongosh mongodb-database-tools
      
      - name: Wait for MongoDB to be ready
        run: |
          echo "Waiting for MongoDB to be ready..."
          for i in {1..30}; do
            if mongosh --host localhost --port 27017 --eval "db.runCommand({ ping: 1 })" > /dev/null 2>&1; then
              echo "MongoDB is ready!"
              break
            fi
            echo "Waiting... ($i/30)"
            sleep 2
          done
      
      - name: Initialize MongoDB with test data
        run: |
          # Import all test data files
          for file in tests/data/*.json; do
            collection=$(basename "$file" .json)
            echo "Importing $collection from $file..."
            
            # Check if file has actual data (not just empty array)
            if grep -q '"' "$file"; then
              mongoimport --host localhost --port 27017 --db metasync-test --collection "$collection" --file "$file" --jsonArray
            else
              echo "  Skipping $collection (empty array)"
            fi
          done
          
          # Create index on priority field in jobs collection
          echo "Creating index on jobs.priority..."
          mongosh --host localhost --port 27017 metasync-test --eval 'db.jobs.createIndex({ priority: -1 })'
          
          # Verify data was loaded
          echo "Verifying data load..."
          mongosh --host localhost --port 27017 metasync-test --eval 'db.getCollectionNames().forEach(function(col) { print(col + ": " + db[col].countDocuments()); })'
      
      - name: Build Docker image
        run: |
          docker build -t metasync:test .
      
      - name: Start metasync container
        run: |
          # Start container using env file (with absolute path for reliability)
          docker run -d \
            --name metasync \
            --network host \
            --env-file "${GITHUB_WORKSPACE}/tests/pipeline-run.env" \
            metasync:test
      
      - name: Wait for metasync to be healthy
        run: |
          echo "Waiting for metasync to be healthy..."
          for i in {1..60}; do
            if curl -f http://localhost:8001/health > /dev/null 2>&1; then
              echo "Metasync is healthy!"
              curl http://localhost:8001/health
              break
            fi
            echo "Waiting... ($i/60)"
            sleep 2
          done
          
          # Final check
          if ! curl -f http://localhost:8001/health > /dev/null 2>&1; then
            echo "Metasync failed to become healthy"
            echo "Container logs:"
            docker logs metasync
            exit 1
          fi
      
      - name: Set up Node.js for Bruno
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Install Bruno CLI
        run: |
          npm install -g @usebruno/cli
      
      - name: Run Bruno tests
        run: |
          echo "Running Bruno test collection..."
          
          # Create results directory and summary file
          mkdir -p test-results
          touch /tmp/test-summary.txt
          touch /tmp/test-detail.txt
          
          # Run tests for each folder in the single Bruno collection
          cd tests/bruno
          
          # Get list of test folders (exclude environments and bruno.json)
          for test_folder in */; do
            # Remove trailing slash
            test_folder=${test_folder%/}
            
            # Skip the environments folder
            if [ "$test_folder" = "environments" ]; then
              continue
            fi
            
            echo "Running tests for: ${test_folder}"
            
            # Run Bruno tests for this specific folder and capture output
            if bru run --env local \
              -r "$test_folder" \
              --output ../../test-results/results-${test_folder}.json 2>&1 | tee /tmp/${test_folder}.log; then
              
              echo ""
              echo "=== DEBUG: First 20 lines of ${test_folder}.log ==="
              head -n 20 /tmp/${test_folder}.log
              echo "=== END DEBUG ==="
              echo ""
              
              echo "‚úÖ ${test_folder}" >> /tmp/test-summary.txt
              echo "‚úÖ ${test_folder}: All tests passed" >> /tmp/test-detail.txt
              # Show all tests for passed suites - look for .bru files
              grep -oE "[0-9]+-[^[:space:]]+\.bru" /tmp/${test_folder}.log | sed 's/\.bru$//' | while read test_name; do
                echo "  ‚úÖ $test_name" >> /tmp/test-detail.txt
              done
              echo "" >> /tmp/test-detail.txt
            else
              echo ""
              echo "=== DEBUG: First 20 lines of ${test_folder}.log ==="
              head -n 20 /tmp/${test_folder}.log
              echo "=== END DEBUG ==="
              echo ""
              
              echo "‚ùå ${test_folder}" >> /tmp/test-summary.txt
              echo "‚ùå ${test_folder}: Some tests failed" >> /tmp/test-detail.txt
              
              # Parse test results - check if test has assertion errors
              current_test=""
              test_failed=false
              
              while IFS= read -r line; do
                # Look for test file names (*.bru)
                if [[ "$line" =~ ([0-9]+-[^[:space:]]+\.bru) ]]; then
                  # Output previous test result if exists
                  if [ -n "$current_test" ]; then
                    if [ "$test_failed" = true ]; then
                      echo "  ‚ùå $current_test" >> /tmp/test-detail.txt
                    else
                      echo "  ‚úÖ $current_test" >> /tmp/test-detail.txt
                    fi
                  fi
                  # Start new test (remove .bru extension)
                  current_test="${BASH_REMATCH[1]%.bru}"
                  test_failed=false
                # Check for assertion errors
                elif echo "$line" | grep -qE "AssertionError|Error:|failed|‚úñ"; then
                  test_failed=true
                fi
              done < /tmp/${test_folder}.log
              
              # Output last test
              if [ -n "$current_test" ]; then
                if [ "$test_failed" = true ]; then
                  echo "  ‚ùå $current_test" >> /tmp/test-detail.txt
                else
                  echo "  ‚úÖ $current_test" >> /tmp/test-detail.txt
                fi
              fi
              
              echo "" >> /tmp/test-detail.txt
            fi
          done
          
          # Return to project root
          cd - > /dev/null
          
          # Generate JUnit XML files from the test results
          echo "Generating JUnit XML files..."
          for test_folder in tests/bruno/*/; do
            test_folder=${test_folder%/}
            folder_name=$(basename "$test_folder")
            
            # Skip environments folder
            if [ "$folder_name" = "environments" ]; then
              continue
            fi
            
            if [ -f /tmp/${folder_name}.log ]; then
              # Count tests and parse results
              total_tests=0
              failed_tests=0
              passed_tests=0
              
              # Start XML file
              echo '<?xml version="1.0" encoding="UTF-8"?>' > test-results/results-${folder_name}.xml
              echo "<testsuite name=\"${folder_name}\" tests=\"0\" failures=\"0\" errors=\"0\" time=\"0\">" >> test-results/results-${folder_name}.xml
              
              # Parse log file for test results
              current_test=""
              test_failed=false
              
              while IFS= read -r line; do
                # Look for test file names (*.bru)
                if [[ "$line" =~ ([0-9]+-[^[:space:]]+\.bru) ]]; then
                  # Save previous test if exists
                  if [ -n "$current_test" ]; then
                    if [ "$test_failed" = true ]; then
                      echo "  <testcase name=\"${current_test}\" classname=\"${folder_name}\" time=\"0\">" >> test-results/results-${folder_name}.xml
                      echo "    <failure message=\"Test failed\">Assertion error</failure>" >> test-results/results-${folder_name}.xml
                      echo "  </testcase>" >> test-results/results-${folder_name}.xml
                      failed_tests=$((failed_tests + 1))
                    else
                      echo "  <testcase name=\"${current_test}\" classname=\"${folder_name}\" time=\"0\"/>" >> test-results/results-${folder_name}.xml
                      passed_tests=$((passed_tests + 1))
                    fi
                    total_tests=$((total_tests + 1))
                  fi
                  
                  # Start new test
                  current_test="${BASH_REMATCH[1]%.bru}"
                  test_failed=false
                  
                # Check for errors/failures
                elif echo "$line" | grep -qE "AssertionError|Error:|failed|‚úñ"; then
                  test_failed=true
                fi
              done < /tmp/${folder_name}.log
              
              # Save last test
              if [ -n "$current_test" ]; then
                if [ "$test_failed" = true ]; then
                  echo "  <testcase name=\"${current_test}\" classname=\"${folder_name}\" time=\"0\">" >> test-results/results-${folder_name}.xml
                  echo "    <failure message=\"Test failed\">Assertion error</failure>" >> test-results/results-${folder_name}.xml
                  echo "  </testcase>" >> test-results/results-${folder_name}.xml
                  failed_tests=$((failed_tests + 1))
                else
                  echo "  <testcase name=\"${current_test}\" classname=\"${folder_name}\" time=\"0\"/>" >> test-results/results-${folder_name}.xml
                  passed_tests=$((passed_tests + 1))
                fi
                total_tests=$((total_tests + 1))
              fi
              
              # Close testsuite with correct counts
              echo "</testsuite>" >> test-results/results-${folder_name}.xml
              
              # Update the counts in the file
              sed -i "s/tests=\"0\"/tests=\"${total_tests}\"/" test-results/results-${folder_name}.xml
              sed -i "s/failures=\"0\"/failures=\"${failed_tests}\"/" test-results/results-${folder_name}.xml
              
              echo "Generated XML for ${folder_name}: ${passed_tests}/${total_tests} passed"
            fi
          done
          
          echo ""
          echo "=========================================="
          echo "TEST SUMMARY"
          echo "=========================================="
          cat /tmp/test-summary.txt
          echo ""
          echo "=========================================="
          echo "DETAILED RESULTS"
          echo "=========================================="
          cat /tmp/test-detail.txt
          echo "üìÑ Full details available in HTML report artifacts"
          echo "=========================================="
      
      - name: Show container logs
        if: always()
        run: |
          echo "=== Metasync Container Logs (Last 100 lines) ==="
          docker logs --tail 100 metasync
          echo ""
          echo "=== MongoDB Connection Test ==="
          mongosh --host localhost --port 27017 metasync-test --eval 'db.getCollectionNames()'
          echo ""
          echo "=== MongoDB Collections Content ==="
          mongosh --host localhost --port 27017 metasync-test --eval 'db.clients.countDocuments()'
          mongosh --host localhost --port 27017 metasync-test --eval 'db.models.countDocuments()'
      
      - name: Generate combined HTML report index
        if: always()
        run: |
          cat > test-results/index.html << 'EOF'
          <!DOCTYPE html>
          <html lang="en">
          <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Metasync Test Results</title>
            <style>
              body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
                background: #f5f5f5;
              }
              h1 {
                color: #333;
                border-bottom: 3px solid #4CAF50;
                padding-bottom: 10px;
              }
              .test-suite {
                background: white;
                border-radius: 8px;
                padding: 20px;
                margin: 15px 0;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                transition: transform 0.2s;
              }
              .test-suite:hover {
                transform: translateY(-2px);
                box-shadow: 0 4px 8px rgba(0,0,0,0.15);
              }
              .test-suite h2 {
                margin-top: 0;
                color: #2196F3;
              }
              .test-suite a {
                display: inline-block;
                padding: 10px 20px;
                background: #2196F3;
                color: white;
                text-decoration: none;
                border-radius: 4px;
                transition: background 0.3s;
              }
              .test-suite a:hover {
                background: #1976D2;
              }
              .timestamp {
                color: #666;
                font-size: 0.9em;
              }
            </style>
          </head>
          <body>
            <h1>üß™ Metasync API Test Results</h1>
            <p class="timestamp">Generated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')</p>
            <div class="test-suites">
          EOF
          
          # Add links to each test suite JSON result
          for result_file in test-results/results-*.json; do
            if [ -f "$result_file" ]; then
              suite_name=$(basename "$result_file" .json | sed 's/results-//')
              cat >> test-results/index.html << EOF
              <div class="test-suite">
                <h2>üìã ${suite_name}</h2>
                <a href="$(basename "$result_file")">View JSON Results ‚Üí</a>
              </div>
          EOF
            fi
          done
          
          cat >> test-results/index.html << 'EOF'
            </div>
          </body>
          </html>
          EOF
          
          echo "Generated test report index at test-results/index.html"
      
      - name: Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            test-results/results-*.xml
          check_name: Test Results
      
      - name: Upload HTML test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-reports-html
          path: test-results/
          retention-days: 30
      
      - name: Add test suites result overview
        if: always()
        run: |
          echo "## üìä Test Suites Result Overview" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add quick summary
          echo "### üìã Test Suites Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f /tmp/test-summary.txt ]; then
            cat /tmp/test-summary.txt >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Add detailed results for each suite
          echo "### üìù Detailed Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Process each test folder in the Bruno collection
          for test_folder in tests/bruno/*/; do
            test_folder=${test_folder%/}
            folder_name=$(basename "$test_folder")
            
            # Skip the environments folder
            if [ "$folder_name" = "environments" ]; then
              continue
            fi
            
            # Add collapsible section for each suite
            echo "<details>" >> $GITHUB_STEP_SUMMARY
            
            # Determine if suite passed or failed
            if grep -q "^‚úÖ ${folder_name}$" /tmp/test-summary.txt 2>/dev/null; then
              echo "<summary>‚úÖ <strong>${folder_name}</strong> - All tests passed</summary>" >> $GITHUB_STEP_SUMMARY
            else
              echo "<summary>‚ùå <strong>${folder_name}</strong> - Some tests failed</summary>" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            
            # Extract and show individual test results based on actual pass/fail
            if [ -f /tmp/${folder_name}.log ]; then
              current_test=""
              test_failed=false
              
              while IFS= read -r line; do
                # Look for test file names (*.bru)
                if [[ "$line" =~ ([0-9]+-[^[:space:]]+\.bru) ]]; then
                  if [ -n "$current_test" ]; then
                    if [ "$test_failed" = true ]; then
                      echo "‚ùå $current_test" >> $GITHUB_STEP_SUMMARY
                    else
                      echo "‚úÖ $current_test" >> $GITHUB_STEP_SUMMARY
                    fi
                  fi
                  # Start new test (remove .bru extension)
                  current_test="${BASH_REMATCH[1]%.bru}"
                  test_failed=false
                elif echo "$line" | grep -qE "AssertionError|Error:|failed|‚úñ"; then
                  test_failed=true
                fi
              done < /tmp/${folder_name}.log
              
              if [ -n "$current_test" ]; then
                if [ "$test_failed" = true ]; then
                  echo "‚ùå $current_test" >> $GITHUB_STEP_SUMMARY
                else
                  echo "‚úÖ $current_test" >> $GITHUB_STEP_SUMMARY
                fi
              fi
            fi
            
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          done
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üì• Download Full HTML Reports" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Full detailed reports with assertions and error messages are available in the **test-reports-html** artifact." >> $GITHUB_STEP_SUMMARY
      
      - name: Cleanup
        if: always()
        run: |
          docker stop metasync || true
          docker rm metasync || true

